{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon_ml_7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D10jZVv2yE8K",
        "outputId": "c030aa4c-4a54-4ae8-9381-e22c1b163a3c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg9s-RZiyHWO",
        "outputId": "9cef4c3e-7b64-4993-a7be-2484ec4e8198"
      },
      "source": [
        "cd /content/drive/MyDrive/amazon_ml/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/amazon_ml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVExUuWZyI3i",
        "outputId": "2775d47e-397a-4c11-d0c0-58a69688bcd1"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import scipy\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from random import shuffle\n",
        "import gc\n",
        "\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_-txccF_J6_"
      },
      "source": [
        "BASE_DIR = ''\n",
        "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
        "TRAIN_DATA_FILE = BASE_DIR + 'dataset/train.csv'\n",
        "TEST_DATA_FILE = BASE_DIR + 'dataset/test.csv'\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.2\n",
        "TEMP_PATH = 'prepro_data2/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipMqWQKo_OHp"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owf2WEm6yQQw",
        "outputId": "e1a2f53e-cf7a-4adb-bfaa-39daea862e93"
      },
      "source": [
        "# not needed\n",
        "print('Indexing word vectors')\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
        "\n",
        "print('Processing text dataset')\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "w2v_set = set(word2vec.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors\n",
            "Found 3000000 word vectors of word2vec\n",
            "Processing text dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCq-thkVygdj",
        "outputId": "6287be9c-12c0-4427-cd98-42332d8f7e18"
      },
      "source": [
        "# not needed\n",
        "train_df = pd.read_csv(TRAIN_DATA_FILE,escapechar='\\\\',quoting=csv.QUOTE_NONE)\n",
        "print(train_df.shape)\n",
        "\n",
        "test_df = pd.read_csv(TEST_DATA_FILE,escapechar='\\\\',quoting=csv.QUOTE_NONE)\n",
        "print(test_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2903024, 5)\n",
            "(110775, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k19OtSwh3EdE"
      },
      "source": [
        "# checkpointed for 3 columns - not needed\n",
        "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]',' ',text.lower().replace('_',' '))\n",
        "    text = re.sub(r'(\\b(\\w*)_(\\w*)\\b)|(\\b(\\w*)(\\d+)(\\w*)\\b)|\\b(\\w*)[^\\w\\s](\\w*)\\b','',text)\n",
        "\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    text = text.split()\n",
        "    if remove_stopwords:\n",
        "        text = [w for w in text if w not in stops]\n",
        "    text = [w for w in text if w in w2v_set and len(w)>2]\n",
        "    text = \" \".join(text)\n",
        "    text = text.split()\n",
        "    text = ' '.join(text)\n",
        "    return(text)\n",
        "\n",
        "for col in ['TITLE','DESCRIPTION','BULLET_POINTS']:\n",
        "  train_df[col].fillna('',inplace=True)\n",
        "  train_df[col] = train_df[col].apply(lambda x:text_to_wordlist(x,remove_stopwords=True, stem_words=False))\n",
        "  test_df[col].fillna('',inplace=True)\n",
        "  test_df[col] = test_df[col].apply(lambda x:text_to_wordlist(x,remove_stopwords=True, stem_words=False))\n",
        "\n",
        "train_df.to_csv(TEMP_PATH+'train_data_cleaned.csv',index=False)\n",
        "test_df.to_csv(TEMP_PATH+'test_data_cleaned.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxY7wc2Y8XIn"
      },
      "source": [
        "# CLEANED DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk81eShm7fMg"
      },
      "source": [
        "train_df = pd.read_csv(TEMP_PATH+'train_data_cleaned.csv')\n",
        "test_df = pd.read_csv(TEMP_PATH+'test_data_cleaned.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXr5n4qZ7U4i"
      },
      "source": [
        "train_df.fillna('',inplace=True)\n",
        "test_df.fillna('',inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZasxpoXP9IFA",
        "outputId": "e79d5177-e998-488d-ef9b-8fbc158ef778"
      },
      "source": [
        "for col in ['TITLE','DESCRIPTION','BULLET_POINTS']:\n",
        "  train_df['LEN_'+col] = train_df[col].apply(lambda x:len(x.split()))\n",
        "  test_df['LEN_'+col] = test_df[col].apply(lambda x:len(x.split()))\n",
        "  print(col,' done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TITLE  done\n",
            "DESCRIPTION  done\n",
            "BULLET_POINTS  done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXHlsdnQ_rzn",
        "outputId": "c15467f8-ab9f-427b-c672-245ed24be9b6"
      },
      "source": [
        "print(np.percentile(train_df.loc[train_df['LEN_TITLE']>0,'LEN_TITLE'].values,99))\n",
        "print(np.percentile(train_df.loc[train_df['LEN_DESCRIPTION']>0,'LEN_DESCRIPTION'].values,90))\n",
        "print(np.percentile(train_df.loc[train_df['LEN_BULLET_POINTS']>0,'LEN_BULLET_POINTS'].values,90))\n",
        "print()\n",
        "print(np.percentile(test_df.loc[test_df['LEN_TITLE']>0,'LEN_TITLE'].values,99))\n",
        "print(np.percentile(test_df.loc[test_df['LEN_DESCRIPTION']>0,'LEN_DESCRIPTION'].values,90))\n",
        "print(np.percentile(test_df.loc[test_df['LEN_BULLET_POINTS']>0,'LEN_BULLET_POINTS'].values,90))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25.0\n",
            "125.0\n",
            "88.0\n",
            "\n",
            "25.0\n",
            "132.0\n",
            "98.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9xKEeu0CwPO",
        "outputId": "ec509269-6710-4c2b-e6d4-12cbf2c97edd"
      },
      "source": [
        "BRACKETS = [25,125,90]\n",
        "\n",
        "def final_cleaner(x,brack):\n",
        "  x = x.split()\n",
        "  if len(x)>brack:\n",
        "    res = []\n",
        "    [res.append(i) for i in x if i not in res]\n",
        "    res = res[:brack]\n",
        "    return ' '.join(res)\n",
        "  else:\n",
        "    return ' '.join(x)\n",
        "\n",
        "for i, col in enumerate(['TITLE','DESCRIPTION','BULLET_POINTS']):\n",
        "  train_df[col] = train_df[col].apply(lambda x:final_cleaner(x,BRACKETS[i]))\n",
        "  test_df[col] = test_df[col].apply(lambda x:final_cleaner(x,BRACKETS[i]))\n",
        "  print(col,' done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TITLE  done\n",
            "DESCRIPTION  done\n",
            "BULLET_POINTS  done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTNPz9wBG4-V",
        "outputId": "e5b9d797-b9f8-4d9a-c844-1a2c52f00fd0"
      },
      "source": [
        "for col in ['TITLE','DESCRIPTION','BULLET_POINTS']:\n",
        "  train_df['LEN_'+col] = train_df[col].apply(lambda x:len(x.split()))\n",
        "  test_df['LEN_'+col] = test_df[col].apply(lambda x:len(x.split()))\n",
        "  print(col,' done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TITLE  done\n",
            "DESCRIPTION  done\n",
            "BULLET_POINTS  done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFabAYENG7b9",
        "outputId": "3955b5cf-3b48-477f-db6a-2cc1445fa055"
      },
      "source": [
        "print(np.percentile(train_df.loc[train_df['LEN_TITLE']>0,'LEN_TITLE'].values,100))\n",
        "print(np.percentile(train_df.loc[train_df['LEN_DESCRIPTION']>0,'LEN_DESCRIPTION'].values,100))\n",
        "print(np.percentile(train_df.loc[train_df['LEN_BULLET_POINTS']>0,'LEN_BULLET_POINTS'].values,100))\n",
        "print()\n",
        "print(np.percentile(test_df.loc[test_df['LEN_TITLE']>0,'LEN_TITLE'].values,100))\n",
        "print(np.percentile(test_df.loc[test_df['LEN_DESCRIPTION']>0,'LEN_DESCRIPTION'].values,100))\n",
        "print(np.percentile(test_df.loc[test_df['LEN_BULLET_POINTS']>0,'LEN_BULLET_POINTS'].values,100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25.0\n",
            "125.0\n",
            "90.0\n",
            "\n",
            "25.0\n",
            "125.0\n",
            "90.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aeWjQO8ye9p"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(np.squeeze(np.vstack([train_df['TITLE'].values[:,np.newaxis],test_df['TITLE'].values[:,np.newaxis],\n",
        "                                             train_df['DESCRIPTION'].values[:,np.newaxis],test_df['DESCRIPTION'].values[:,np.newaxis],\n",
        "                                             train_df['BULLET_POINTS'].values[:,np.newaxis],test_df['BULLET_POINTS'].values[:,np.newaxis]])))\n",
        "print(len(tokenizer.word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZGfNt2jxvqq"
      },
      "source": [
        "if os.path.exists(TEMP_PATH):\n",
        "  pass\n",
        "else:\n",
        "  os.makedirs(TEMP_PATH,exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trWxhqm3aK2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49cb35cd-7662-4740-dde7-d52a1a3f9fae"
      },
      "source": [
        "for i,col in enumerate(['TITLE','DESCRIPTION','BULLET_POINTS']):\n",
        "  data = tokenizer.texts_to_sequences(train_df[col].values)\n",
        "  data_test = tokenizer.texts_to_sequences(test_df[col].values)\n",
        "  data = pad_sequences(data, maxlen=BRACKETS[i])\n",
        "  data_test = pad_sequences(data_test, maxlen=BRACKETS[i])\n",
        "\n",
        "  print('Shape of data:', data.shape)\n",
        "  print('Shape of data_test:', data_test.shape)\n",
        "\n",
        "  with open(TEMP_PATH+'data_{}.npy'.format(i), 'wb') as f:\n",
        "      np.save(f, data)\n",
        "  with open(TEMP_PATH+'data_test_{}.npy'.format(i), 'wb') as f:\n",
        "      np.save(f, data_test)\n",
        "\n",
        "enc = LabelEncoder()\n",
        "Y = train_df['BROWSE_NODE_ID'].values\n",
        "Y = enc.fit_transform(Y)\n",
        "print(Y.shape)\n",
        "print(enc.classes_.shape)\n",
        "\n",
        "with open(TEMP_PATH+'labels.npy', 'wb') as f:\n",
        "    np.save(f, Y)\n",
        "with open(TEMP_PATH+'labels_mapping.npy', 'wb') as f:\n",
        "    np.save(f, enc.classes_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data: (2903024, 25)\n",
            "Shape of data_test: (110775, 25)\n",
            "Shape of data: (2903024, 125)\n",
            "Shape of data_test: (110775, 125)\n",
            "Shape of data: (2903024, 90)\n",
            "Shape of data_test: (110775, 90)\n",
            "(2903024,)\n",
            "(9919,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJUbU4A4l-bn",
        "outputId": "621133a8-d655-4989-bab1-5ef981cbebd3"
      },
      "source": [
        "del data\n",
        "del data_test\n",
        "del Y\n",
        "del enc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHY9D447z5HI",
        "outputId": "c5cfe0e5-e9d9-4d03-8073-d4ed170e77fa"
      },
      "source": [
        "del train_df\n",
        "del test_df\n",
        "del train_df_col\n",
        "del test_df_col\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f87zY4Q3mT0S",
        "outputId": "c8171bae-c702-407b-909b-af0a3d8ae55d"
      },
      "source": [
        "print('Indexing word vectors')\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors\n",
            "Found 3000000 word vectors of word2vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQkyh3bn4hq6",
        "outputId": "d3ce0e3b-84bd-4c93-9cae-6286a8c1a454"
      },
      "source": [
        "nb_words = len(tokenizer.word_index)+1\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "      embedding_matrix[i] = word2vec.word_vec(word)\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 1\n",
            "(90766, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THQA5IFw4tQ4"
      },
      "source": [
        "with open(TEMP_PATH+'embeddings.npy', 'wb') as f:\n",
        "    np.save(f, embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf3r36g27Hf1",
        "outputId": "b85e60e0-b225-496a-e02c-ab12c0c2123a"
      },
      "source": [
        "with open(TEMP_PATH+'data_0.npy', 'rb') as f:\n",
        "    data_0 = np.load(f)\n",
        "\n",
        "with open(TEMP_PATH+'data_test_0.npy', 'rb') as f:\n",
        "    data_test_0 = np.load(f)\n",
        "\n",
        "with open(TEMP_PATH+'data_1.npy', 'rb') as f:\n",
        "    data_1 = np.load(f)\n",
        "\n",
        "with open(TEMP_PATH+'data_test_1.npy', 'rb') as f:\n",
        "    data_test_1 = np.load(f)\n",
        "\n",
        "with open(TEMP_PATH+'data_2.npy', 'rb') as f:\n",
        "    data_2 = np.load(f)\n",
        "\n",
        "with open(TEMP_PATH+'data_test_2.npy', 'rb') as f:\n",
        "    data_test_2 = np.load(f)\n",
        "\n",
        "data = np.concatenate([data_0, data_1, data_2], axis=1)\n",
        "print(data.shape)\n",
        "data_test = np.concatenate([data_test_0, data_test_1, data_test_2], axis=1)\n",
        "print(data_test.shape)\n",
        "\n",
        "with open(TEMP_PATH+'data.npy', 'wb') as f:\n",
        "    np.save(f, data)\n",
        "with open(TEMP_PATH+'data_test.npy', 'wb') as f:\n",
        "    np.save(f, data_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2903024, 240)\n",
            "(110775, 240)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdmxNjyT_eMP"
      },
      "source": [
        "# LOADING PREPROCESSED\n",
        "* Start from here\n",
        "* 58005, 2903024, 110775, 300, 82, 9919"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tNjukwk8wkH",
        "outputId": "7ea8dca8-4788-41a0-f72f-1ae5ba4eb294"
      },
      "source": [
        "# loading preproceesed\n",
        "\n",
        "with open(TEMP_PATH+'embeddings.npy', 'rb') as f:\n",
        "    embedding_matrix = np.load(f)\n",
        "\n",
        "with open(TEMP_PATH+'labels.npy', 'rb') as f:\n",
        "    Y = np.load(f)\n",
        "\n",
        "with open(TEMP_PATH+'labels_mapping.npy', 'rb') as f:\n",
        "    enc_classes = np.load(f)\n",
        "\n",
        "with open(TEMP_PATH+'data.npy', 'rb') as f:\n",
        "    data = np.load(f)\n",
        "\n",
        "with open(TEMP_PATH+'data_test.npy', 'rb') as f:\n",
        "    data_test = np.load(f)\n",
        "\n",
        "print(embedding_matrix.shape)\n",
        "print(Y.shape)\n",
        "print(enc_classes.shape)\n",
        "print(data.shape)\n",
        "print(data_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90766, 300)\n",
            "(2903024,)\n",
            "(9919,)\n",
            "(2903024, 240)\n",
            "(110775, 240)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8HafiRHAJMu"
      },
      "source": [
        "MODEL_NUM = 6\n",
        "nb_words = embedding_matrix.shape[0]\n",
        "NUM_LABELS = len(enc_classes)\n",
        "VALIDATION_SPLIT = 0.2\n",
        "EMBEDDING_DIM = embedding_matrix.shape[1]\n",
        "MAX_SEQUENCE_LENGTH = 240\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "os.makedirs(f'model-{MODEL_NUM}',exist_ok=True)\n",
        "MODEL_PATH = f'model-{MODEL_NUM}/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDyaV8dsXpxc"
      },
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return K.sum(output,axis=1)\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWKSHMoNDdbn",
        "outputId": "3225d98e-3af4-4285-dc5b-1b01568d490a"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(101)\n",
        "perm = np.random.permutation(len(data))\n",
        "idx_train = perm[:int(len(data)*(1-VALIDATION_SPLIT))]\n",
        "idx_val = perm[int(len(data)*(1-VALIDATION_SPLIT)):]\n",
        "print(len(idx_train), len(idx_val))\n",
        "\n",
        "data_train = data[idx_train]\n",
        "data_val = data[idx_val]\n",
        "Y_train = Y[idx_train]\n",
        "Y_val = Y[idx_val]\n",
        "del data\n",
        "del Y\n",
        "gc.collect()\n",
        "\n",
        "# model\n",
        "embedding_layer = Embedding(nb_words,\n",
        "        EMBEDDING_DIM,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_SEQUENCE_LENGTH,\n",
        "        trainable=False)\n",
        "\n",
        "lstm_layer_1 = LSTM(250, return_sequences=True, dropout=0.15, recurrent_dropout=0.15)\n",
        "#lstm_layer_2 = LSTM(200, return_sequences=False, dropout=0.15, recurrent_dropout=0.15)\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "x1 = embedding_layer(sequence_input)\n",
        "x1 = lstm_layer_1(x1)\n",
        "x1 = attention()(x1)\n",
        "x1 = Dropout(0.10)(x1)\n",
        "x1 = BatchNormalization()(x1)\n",
        "x1 = Dense(10000, activation='relu')(x1)\n",
        "x1 = Dropout(0.10)(x1)\n",
        "x1 = BatchNormalization()(x1)\n",
        "preds = Dense(NUM_LABELS, activation='softmax')(x1)\n",
        "\n",
        "model = Model(inputs=sequence_input, outputs=preds)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2322419 580605\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 240)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 240, 300)          27229800  \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 240, 250)          551000    \n",
            "_________________________________________________________________\n",
            "attention (attention)        (None, 250)               490       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 250)               1000      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10000)             2510000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 10000)             40000     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 9919)              99199919  \n",
            "=================================================================\n",
            "Total params: 129,532,209\n",
            "Trainable params: 102,281,909\n",
            "Non-trainable params: 27,250,300\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FCQhJhM_jRx"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((data_train, Y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=data_train.shape[0]).batch(BATCH_SIZE)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((data_val, Y_val))\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(data_test)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkL8zhcF0jmo"
      },
      "source": [
        "model.load_weights(MODEL_PATH+'epoch:01-loss:1.046-val_acc:0.737.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yzV_fwHDAIz"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred = model(x_batch_train, training=True)\n",
        "            loss_value = loss_fn(y_batch_train, pred)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        train_acc_metric.update_state(y_batch_train, pred)\n",
        "        if step % 20 == 0:\n",
        "            print(\"Step-->{:02d}, Loss-->{:.3f}, Accuracy-->{:.3f}\".format(step+1, loss_value, train_acc_metric.result().numpy()))\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc: %.4f\" % (float(train_acc)))\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_pred = model(x_batch_val, training=False)\n",
        "        val_acc_metric.update_state(y_batch_val, val_pred)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "\n",
        "    ans = []\n",
        "    for x_batch_test in test_dataset:\n",
        "        test_pred = model(x_batch_test, training=False).numpy().argmax(axis=1)\n",
        "        ans.append(test_pred)\n",
        "    ans = np.concatenate(ans)\n",
        "    ans = enc_classes[ans]\n",
        "    sub_df = pd.DataFrame(ans, columns=['BROWSE_NODE_ID'])\n",
        "    sub_df['PRODUCT_ID'] = sub_df.index + 1\n",
        "    sub_df = sub_df[['PRODUCT_ID','BROWSE_NODE_ID']]\n",
        "    sub_df.to_csv(MODEL_PATH+'epoch:{:02d}-val_acc:{:.3f}-sub.csv'.format(epoch,val_acc), index=False)\n",
        "    print(\"Output saved for test set.\")\n",
        "    model.save_weights(MODEL_PATH+\"epoch:{:02d}-loss:{:.3f}-val_acc:{:.3f}.h5\".format(epoch, loss_value.numpy(), float(val_acc)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMxB58Ks4Dfa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}